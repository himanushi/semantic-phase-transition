# 実験6: Universal Weight Subspace — W_QK の PCA 圧縮と機能的影響 — 結果レポート

## 実験条件

- **対象**: 全 attention head の W_QK = W_Q.T @ W_K (d_head × d_head)
- **モデル**: GPT-2 small (12層 × 12ヘッド = 144ヘッド, d_head=64)
- **PCA 入力**: (144, 4096) 行列（各 W_QK を flatten）
- **Perplexity 評価**: WikiText-2 test set, 195チャンク × 1024トークン
- **デバイス**: MPS (Apple Silicon)
- **実行日**: 2026-02-19

---

## 主要な発見

### 1. PCA は重み空間の構造を捉える（6A）

W_QK の累積寄与率:

| 閾値 | 基底数 K | 全体 (144) に対する比率 |
|------|---------|----------------------|
| 50%  | 9       | 6.3%                 |
| 80%  | 54      | 37.5%                |
| 90%  | 82      | 56.9%                |
| 95%  | 103     | 71.5%                |

上位3成分だけで分散の 37.3% を説明。UWS 論文の主張（16-32基底で90%）と比較すると、
GPT-2 small の W_QK は**やや分散が分散的**（90%に82基底が必要）。

Top 3 主成分の寄与率: PC1=17.7%, PC2=11.1%, PC3=8.5%。

### 2. PCA 分散 ≠ 機能的重要度（6B）★核心の結果★

Baseline perplexity: **29.79**
K=all (144) 検証: **29.79** (差 0.000003 — 完全一致)

| K | 累積分散 | Perplexity | Ratio | 判定 |
|---|---------|-----------|-------|------|
| 1 | 17.7% | 2034.3 | 68.3x | |
| 3 | 37.3% | 1711.0 | 57.4x | |
| 10 | 52.0% | 1200.9 | 40.3x | |
| 32 | 71.4% | 558.3 | 18.7x | |
| 64 | 87.1% | 329.9 | 11.1x | |
| 80 | 91.0% | 250.0 | 8.4x | |
| 100 | 95.3% | 125.3 | 4.2x | |
| 110 | 96.8% | 84.5 | 2.8x | |
| 120 | 98.2% | 70.3 | 2.4x | |
| 130 | 99.1% | 64.2 | 2.2x | |
| **140** | **99.7%** | **32.0** | **1.07x** | 許容範囲 |
| 144 | 100% | 29.8 | 1.00x | Baseline |

**結論**: 分散の上位 95%（K=100）を残しても perplexity は **4.2 倍**。
K=140/144（たった4成分を削除）でようやく **1.07 倍**。

特筆すべきは K=130 → K=140 での急激な回復（2.2x → 1.07x）。
PC131-PC140 が説明する分散はわずか 0.6% だが、perplexity への影響は劇的。

### 3. ヘッドクラスタリングも壊滅的（6C）

PCA 係数空間（上位20次元）で k-means クラスタリングし、
各クラスタの代表ヘッドで同クラスタ内の全ヘッドを置換:

| k | 置換ヘッド数 | Perplexity | Ratio |
|---|------------|-----------|-------|
| 4 | 140 | 1351.9 | 45.4x |
| 8 | 136 | 2208.7 | 74.2x |
| 12 | 132 | 1092.3 | 36.7x |
| 16 | 128 | 1242.5 | 41.7x |
| 32 | 112 | 830.4 | 27.9x |
| 64 | 80 | 482.7 | 16.2x |

k=8 が k=4 より悪い（非単調性）のは、k-means のクラスタ割り当てにより
重要なヘッドが不適切なクラスタに分類されたため。

### 4. PCA 2D 可視化: レイヤー依存構造

PCA の上位2成分（累積 28.9%）に射影すると:
- **レイヤーインデックスに沿った勾配**: PC1 方向に浅い層 → 深い層が系統的にシフト
- **外れ値**: 初期レイヤー（Layer 0-1）に2-3個の孤立点
- **中心クラスター**: 中間〜深いレイヤーのヘッドが密集

W_QK のバリエーションは主にレイヤー位置で決まるが、
同一レイヤー内のヘッド間変動も無視できない。

---

## 解釈

### PCA 分散と機能的重要度の乖離

この結果は UWS 仮説の根本的な限界を示す:

1. **PCA は L2 ノルムに基づく分散を最大化**する。大きなスケールの変動は
   モデルの「構造的な個性」（レイヤー位置依存性など）を反映し、
   必ずしも推論に重要な微細構造ではない。

2. **低分散の主成分が perplexity に決定的に寄与**する。
   PC131-PC140（分散 0.6%）の除去で perplexity が 2.2x → 1.07x に変化。
   これは重み空間の「小さな方向」に機能的に重要な情報が符号化されていることを意味する。

3. **分散比率 → 性能比率の変換は指数的**（非線形）。
   分散 95% → PPL 4.2x, 分散 99.7% → PPL 1.07x。
   残り 5% の分散が perplexity の 95% を決めている。

### クラスタリングの失敗

ヘッドクラスタリングが PCA 再構成より悪いのは:
- PCA 再構成は**全ヘッドを同じ部分空間に射影**（共通の低ランク近似）
- クラスタリングは**代表ヘッドで完全に置換**（離散的な近似）
- 各ヘッドの W_QK は高次元空間で十分に分離しており、代表で近似不可能

### 先行研究との関係

UWS 論文 (Kaushik et al., 2025) は「16-32基底で分散の90%を説明」を示したが、
本実験は「分散の90%説明 ≠ perplexity の維持」を実証。
彼らの分析は**重み空間の統計的構造**に焦点を当てており、
**機能的影響の評価**（perplexity 測定）は行っていない。

---

## README の予測パターンとの対応

```
最良: K=10でperplexity 1.05倍以内    → ✗ (K=10 で 40x)
良好: K=16-32で1.1倍以内            → ✗ (K=32 で 19x)
微妙: K=64でも1.2倍超              → ✓ (K=64 で 11x)
最悪: PCAで構造が見えない            → △ (構造は見えるが圧縮は不可能)
```

実質的には「微妙」と「最悪」の間。PCA は重み空間の構造を捉えるが、
その構造に基づく圧縮は機能を破壊する。

---

## Weight injection の技術的詳細

W_QK_recon を元モデルに注入する際の正しい数式:

```
W_Q_new = W_Q_orig + W_K @ inv(W_K.T @ W_K) @ (W_QK_recon - W_QK_orig).T
```

これは以下を保証する:
- W_Q_new.T @ W_K = W_QK_recon（所望の QK 回路）
- K=all のとき delta=0 なので W_Q_new = W_Q_orig（完全一致、差 0.000003 で検証済み）
- W_Q の null(W_K.T) 方向の成分を保存（attention score に無関係な成分を変えない）

pseudoinverse 解 `W_Q_new = W_K @ inv(W_K.T W_K) @ W_QK_recon.T` は
null-space 成分を捨てるため、K=all でも baseline と一致しない。

---

## 6D (GPT-2 medium) について

K=10 で ratio=40x であり、事前に設定した基準
「K=10 で perplexity 1.1 倍以内」を大幅に超えるため、
GPT-2 medium での実行は省略。

---

## 結論

1. **UWS の「分散の90%を少数基底で説明」は推論品質の維持に不十分**。
   GPT-2 small では K=140/144 でようやく PPL 1.07x。
2. **低分散の主成分が機能的に決定的**。分散 0.6% の成分で PPL が 2x 変化。
3. **ヘッドクラスタリングはさらに困難**。k=64 (56% ユニーク) でも PPL 16x。
4. **W_QK の PCA に基づく圧縮は GPT-2 では実用不可能**。
5. **PCA 2D 可視化ではレイヤー依存構造が確認**されたが、機能的冗長性は見られない。

---

## 生成ファイル

### データ
- `results/data/exp6_gpt2.json` — 全数値データ（6A/6B/6C）

### 図
- `results/figures/exp6a_cumvar_gpt2.png` — 累積寄与率
- `results/figures/exp6b_ppl_vs_K_gpt2.png` — 基底数 vs perplexity
- `results/figures/exp6c_cluster_ppl_gpt2.png` — クラスタ数 vs perplexity
- `results/figures/exp6c_cluster_viz_gpt2.png` — PCA 2D 可視化（レイヤー色分け / クラスタ色分け）
