# Experiment 10: Validation + Phase 3 Results

## Summary

Phase 1-2の全結果を250Kトークン（WikiText-2全体）で再検証し、Phase 3（CoT回収）の実験を実施した。

**主要結論**:

1. **2Kトークン推定は楽観的だが定性的結論は維持**: ベースラインPPLが24.27→31.04に上昇し、ReLU累積閾値は控えめに修正（1.1x: 30→20ヘッド、1.5x: 70→60）。ただし全閾値の変化は10ヘッド以内で、Phase 2の結論は概ね有効。

2. **L0H8の「定常バイアス」発見はむしろ強化**: zero/mean-ablation比が219x→303xに拡大。全トークンでより顕著。

3. **CoT回収は部分的に成功**:
   - **Self-refinement**: 3回反復で全条件がPPL≈1.4に収束。しかしこれは繰り返し生成の退化（greedy decoding の予測可能性）による見かけの収束であり、真の品質改善とは言えない
   - **Best-of-N**: relu_30@N=5 (PPL=8.68) が baseline@N=1 (PPL=13.62) を上回る。生成多様性による品質改善は有効
   - **等FLOPs比較**: kernel化で浮くFLOPsでは追加6-16トークンしか生成できず、best-of-Nの大幅なN増加は困難

4. **コンテキスト長の影響**: relu_30の劣化比率はseq長にほぼ非依存（1.10-1.16x）。relu_70は長コンテキストで改善傾向（1.71x@128 → 1.52x@1024）。

---

## Part V: Validation

### V1: Full-token Baseline

| Metric | 2K tokens | Full tokens (250K) | 変化 |
|--------|:---------:|:------------------:|:----:|
| Chunks | 2 | 245 | 122x |
| Tokens | 2,048 | 250,880 | 122x |
| Baseline PPL | 24.27 | 31.04 | +28% |

2Kトークンの推定はPPLを過小評価していた。これはWikiText-2の先頭部分が比較的予測しやすいテキストを含んでいるため。

### V2: Top-10 Head Zero-Ablation

| Rank(2K) | Rank(Full) | Head | ΔPPL(2K) | ΔPPL(Full) |
|:---------:|:----------:|:----:|:--------:|:----------:|
| 1 | 1 | L0H8 | +2.365 | +2.806 |
| 2 | 3 | L0H1 | +0.734 | +0.634 |
| 3 | 2 | L0H11 | +0.678 | +0.722 |
| 4 | 4 | L0H10 | +0.547 | +0.459 |
| 5 | 5 | L0H5 | +0.247 | +0.254 |
| 6 | 8 | L0H9 | +0.159 | +0.132 |
| 7 | 6 | L11H0 | +0.156 | +0.156 |
| 8 | 7 | L0H3 | +0.131 | +0.146 |
| 9 | 10 | L8H2 | +0.072 | +0.073 |
| 10 | 9 | L0H6 | +0.072 | +0.091 |

- **L0H8が圧倒的1位は不変**（2.81x、2位の3.9倍）
- 上位5位の構成は完全に同一
- 6-10位で微小な入れ替え（L0H9↔L11H0、L8H2↔L0H6）
- **ランキングは安定**: 定性的結論に影響なし

### V3: L0H8 Mean-Ablation

| Metric | 2K tokens | Full tokens | 変化 |
|--------|:---------:|:-----------:|:----:|
| Zero ΔPPL | +2.365 | +2.806 | +19% |
| Mean ΔPPL | +0.011 | +0.009 | -14% |
| Reduction | 219x | **303x** | +38% |

**Phase 1.5の最大発見がさらに強化された。** L0H8の機能の99.7%（2K: 99.5%）が定常バイアスであることが全トークンで確認。

### V4: ReLU Cumulative Linearization

| N linearized | PPL(2K) | Ratio(2K) | PPL(Full) | Ratio(Full) |
|:------------:|:-------:|:---------:|:---------:|:-----------:|
| 0 | 24.27 | 1.000x | 31.04 | 1.000x |
| 10 | 24.47 | 1.008x | 31.76 | 1.023x |
| 20 | 24.80 | 1.022x | 32.91 | 1.060x |
| 30 | 26.39 | 1.087x | 35.18 | 1.133x |
| 40 | 27.71 | 1.142x | 37.34 | 1.203x |
| 50 | 29.45 | 1.214x | 39.71 | 1.279x |
| 60 | 33.14 | 1.366x | 43.88 | 1.414x |
| 70 | 35.94 | 1.481x | 47.70 | 1.537x |
| 80 | 37.23 | 1.534x | 49.82 | 1.605x |
| 90 | 45.64 | 1.881x | 59.61 | 1.921x |
| 100 | 56.57 | 2.331x | 73.66 | 2.373x |
| 132 | 308.0 | 12.69x | 364.8 | 11.75x |

**閾値比較:**

| Threshold | 2K tokens | Full tokens | 変化 |
|:---------:|:---------:|:-----------:|:----:|
| PPL ≤ 1.1x | 30 | 20 | -10 |
| PPL ≤ 1.5x | 70 | 60 | -10 |
| PPL ≤ 2.0x | 90 | 90 | 0 |

2K推定は10ヘッド程度楽観的だったが、定性的な結論（「30前後のヘッドがReLU化可能」「大半のヘッドでReLUは無害」）は維持される。

### V5: 判定

- **バイアス方向**: 2Kトークンは過大推定（楽観的）
- **平均差異**: +24.6%
- **Phase 2結論維持**: **YES**（全閾値変化 ≤ 10ヘッド）

**修正された推奨値**:
- 保守的に **20ヘッド** をReLU化すればPPL 1.1x以内（2K推定: 30）
- 60ヘッドでPPL 1.5x以内（2K推定: 70）

---

## Part P: Phase 3 — CoT Recovery

### P1: Generation Speed

| Condition | tok/s | Latency (ms/tok) | Note |
|-----------|:-----:|:-----------------:|------|
| Baseline | 17.3 | 57.9 | — |
| ReLU-30 | 15.7 | 63.7 | -9.2% |
| ReLU-70 | 15.4 | 65.1 | -11.0% |

**M1 Macでは線形化による速度向上はない。** むしろフックのオーバーヘッドで10%低下。
これはsoftmax→ReLUの置換がPythonフック経由であり、MPSバックエンドにkernel化の恩恵がないため。

**理論的kernel化スピードアップ** (seq=1024):
- 30ヘッド: 1.23x
- 70ヘッド: 1.79x

真の速度改善にはCUDA kernel実装が必要。

### P2: Self-Refinement

| Condition | Iter 1 | Iter 2 | Iter 3 |
|-----------|:------:|:------:|:------:|
| Baseline | 2.44 | 1.65 | 1.42 |
| ReLU-30 | 2.21 | 1.58 | 1.37 |
| ReLU-70 | 2.70 | 1.70 | 1.43 |

**全条件がiter3でPPL≈1.4に収束。** しかしこの結果の解釈には注意が必要:

1. **PPL < 2は生成テキストの退化を示唆**: Greedy decodingで生成したテキストは反復的で予測可能 → 評価PPLが人工的に低い
2. **ReLU-70のgapが消失**: iter1で+0.26だった差がiter3で+0.01に → 反復生成の「退化先」が同一
3. **これは「CoTによる品質回収」ではない**: 単に全モデルが同じ退化パターンに収束しているだけ

**結論**: GPT-2 (non-instruction-tuned) でのself-refinementは品質改善にならない。Instruction-tunedモデルでの再検証が必要。

### P3: Best-of-N

| Condition | N=1 | N=2 | N=3 | N=5 |
|-----------|:---:|:---:|:---:|:---:|
| Baseline | 13.62 | 11.11 | 10.20 | 8.02 |
| ReLU-30 | 17.33 | 12.35 | 11.18 | 8.68 |
| ReLU-70 | 22.79 | 16.47 | 13.09 | 11.38 |

**Best-of-Nは有効な品質回収手段:**

- **ReLU-30@N=5 (8.68) < Baseline@N=1 (13.62)**: 5候補中最良を選べば、ベースラインの直接生成を上回る
- **ReLU-70@N=5 (11.38) < Baseline@N=1 (13.62)**: 70ヘッド線形化でもN=5で回収可能

ただし、等FLOPsでの追加候補数は限定的:
- ReLU-30のkernel化: FLOPs 8.6%削減 → 追加6トークン（候補数増加は困難）
- ReLU-70のkernel化: FLOPs 20%削減 → 追加16トークン

**真の等FLOPs比較**: kernel化で浮くFLOPsでは追加候補生成（N増加）は困難。Best-of-Nの恩恵は追加計算を許容する場合にのみ有効。

### P4: Context Length vs PPL

| seq_len | Baseline | ReLU-30 | Ratio | ReLU-70 | Ratio |
|:-------:|:--------:|:-------:|:-----:|:-------:|:-----:|
| 128 | 55.48 | 64.39 | 1.161x | 94.70 | 1.707x |
| 256 | 42.11 | 46.84 | 1.113x | 62.27 | 1.479x |
| 512 | 33.94 | 37.19 | 1.096x | 46.76 | 1.378x |
| 1024 | 28.98 | 32.52 | 1.122x | 43.97 | 1.518x |

- **ReLU-30**: 劣化比率はseq長にほぼ非依存（1.10-1.16x）。安定。
- **ReLU-70**: 長コンテキストで改善傾向（128: 1.71x → 512: 1.38x）。ただし1024で1.52xに戻る（ノイズの可能性）。

**示唆**: softmax attentionの「精密な確率分布」は短コンテキストでより重要。長コンテキストでは多くのトークンからの情報が冗長になり、ReLUの粗い重み付けでも十分な情報が取れる。

---

## Experimental Setup

- Model: GPT-2 small (12L × 12H, 124M params)
- Dataset: WikiText-2 validation set (full: 245 chunks × 1024 = 250,880 tokens)
- Device: M1 Mac (MPS)
- Part V runtime: ~40 min
- Part P runtime: ~50 min

---

## exp10全体を通じた最終結論

### 仮説5の検証結果

**「Attentionの精密さを下げ、浮いた計算をCoTに回すことで性能を維持できる」**

→ **部分的に棄却。** 以下の理由から:

1. **ReLU化は驚くほど無害**（Phase 2、確認済み）: 20ヘッド（全トークン検証値）がPPL 1.1x以内でReLU化可能。softmaxの確率分布の精密さは多くのヘッドで不要。

2. **しかし浮いた計算は微小**（Phase 3 P1/P3）: kernel化で削減されるattention FLOPsは全体の8-20%。これで追加生成できるトークンは6-16で、有意なCoT/best-of-Nには不十分。

3. **Self-refinementは機能しない**（Phase 3 P2）: GPT-2レベルでは反復生成が退化に向かう。Instruction-tunedモデルでは異なる可能性があるが、本実験では検証不可。

4. **Best-of-Nは有効だが追加計算が必要**（Phase 3 P3）: N=5でReLU-30がbaseline@N=1を上回るが、等FLOPsでの実現は困難。

### 成果の整理

| Phase | 発見 | 意義 |
|-------|------|------|
| Ph1 | L0に機能集中、Zipf α=1.37 | Attentionヘッドの機能は極度に不均等 |
| Ph1.5 | L0H8の99.7%が定常バイアス | ヘッドの「機能」の大部分は入力非依存 |
| Ph2 | ReLU linearizationが最良 | Softmaxの精密さは多くのヘッドで不要 |
| Ph3 V | 2K推定は10ヘッド程度楽観的 | 50K+トークンでの検証は必須 |
| Ph3 P | Best-of-Nは有効、self-refinementは無効 | CoT回収は条件付きで可能 |

### 残された課題

1. **CUDA kernel実装**: hook-based linearizationでは速度向上しない。causal masking付きReLU kernelの実装が必要
2. **Instruction-tunedモデル**: self-refinement/CoTの真価はGPT-2では測定不能。GPT-2-chat相当やLlama-Instructでの検証が必要
3. **スケーリング**: GPT-2 medium/largeでの閾値は異なる可能性（モデルが大きいほど冗長ヘッドが多い可能性）
4. **Finetuning**: ReLU化後のfinetuningで劣化を回収できる可能性（本実験ではpost-hoc置換のみ）

---

## Files

- Data: `results/data/exp10v_validation_gpt2.json`, `results/data/exp10p_phase3_gpt2.json`
- Figures: `results/figures/exp10v_*.png`, `results/figures/exp10p*.png`
- Script: `experiments/exp10d_validation.py`
