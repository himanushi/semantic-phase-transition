# 実験3: 線形応答の限界 — 結果レポート

## 実験条件

- **モデル**: GPT-2 small (12層), GPT-2 medium (24層)
- **デバイス**: MPS (Apple Silicon)
- **対象語**: 4語 (rock, spring, bass, light)
- **プロンプト**: 各語×各方向10段階の文脈勾配（計80プロンプト）
- **外部場 h の定義**: h = σ_final / max|σ_final| で正規化
- **線形性の評価**: 各レイヤーで σ(l) vs h の線形回帰 R²
- **非線形閾値 h\***: 残差が σ range の 5% を超える最小の |h|
- **実行日**: 2026-02-19

---

## 主要な発見

### 1. R² のレイヤー依存性 — 「線形性は深い層ほど良い」

全語・全モデルで共通のパターン:

```
L0 (embedding):  R² ≈ 0.0 (ほぼ非線形 / 無相関)
L1-L3:           R² = 0.7-0.85 (弱い線形性)
L_mid:           R² = 0.90-0.95 (良好な線形性)
L_final:         R² = 1.00 (定義上、h の定義元)
```

| word | GPT-2 small R²>0.95 | GPT-2 medium R²>0.95 |
|------|---------------------|----------------------|
| rock | 6/13 層 | 6/25 層 |
| spring | 9/13 層 | 14/25 層 |
| bass | 10/13 層 | 16/25 層 |
| light | 9/13 層 | 14/25 層 |

**解釈**: embedding 層では文脈情報がまだ到達しておらず、σ は h に無関係。レイヤーが進むにつれ文脈情報が浸透し、σ vs h の関係が線形に収束する。

### 2. f(l) = dσ/dh の普遍性 — 語間相関 > 0.96

最も重要な発見。f(l) の語間相関行列:

**GPT-2 small**:
| | rock | spring | bass | light |
|---|---|---|---|---|
| rock | 1.000 | 0.987 | 0.992 | 0.978 |
| spring | 0.987 | 1.000 | 0.980 | 0.990 |
| bass | 0.992 | 0.980 | 1.000 | 0.966 |
| light | 0.978 | 0.990 | 0.966 | 1.000 |

**GPT-2 medium**:
| | rock | spring | bass | light |
|---|---|---|---|---|
| rock | 1.000 | 0.980 | 0.996 | 0.978 |
| spring | 0.980 | 1.000 | 0.974 | 0.996 |
| bass | 0.996 | 0.974 | 1.000 | 0.970 |
| light | 0.978 | 0.996 | 0.970 | 1.000 |

**全ペアで相関 > 0.96**。これは f(l) の「形状」が語に依存しない普遍関数であることを強く示唆する。語ごとの違いはスケール（f_max）のみ。

### 3. f_max の語間差

| word | GPT-2 small f_max | GPT-2 medium f_max |
|------|------------------|-------------------|
| rock | 0.373 | 0.387 |
| spring | 0.309 | 0.495 |
| bass | 0.303 | 0.500 |
| light | **0.793** | 0.548 |

light が GPT-2 small で突出して大きい。これは light の2つの解釈（illumination vs weight）がモデルにとって意味的に離れている（対比的方向ベクトルの分離が良い）ことを反映している可能性がある。

### 4. h\* の挙動 — 非線形性は初期層に集中

| word | GPT-2 small min h\* | 位置 | GPT-2 medium min h\* | 位置 |
|------|--------------------|----|---------------------|-----|
| rock | 0.070 | L1 | 0.039 | L0 |
| spring | 0.014 | L0 | 0.269 | L0 |
| bass | 0.173 | L0 | 0.262 | L0 |
| light | 0.348 | L0 | 0.092 | L0 |

h\* は全ケースで L0 または L1 で最小。初期層では弱い文脈でも非線形応答を引き起こすが、深い層では線形領域が広い。

---

## 制限事項

### h 範囲の片側偏り

**GPT-2 medium では全4語で h > 0**（B方向プロンプトが σ_final を負にできていない）:

| word | GPT-2 small h range | GPT-2 medium h range |
|------|--------------------|--------------------|
| rock | [-0.31, 1.00] | [0.04, 1.00] |
| spring | [-1.00, 0.64] | [0.27, 1.00] |
| bass | [-1.00, 0.52] | [0.26, 1.00] |
| light | [0.35, 1.00] | [0.09, 1.00] |

**原因**: GPT-2 medium は特定の解釈に対するprior biasが強く、勾配プロンプトの「弱い」方（B方向の step 1-5）では σ_final を負にできない。

**影響**: medium の R² と f(l) は正の h 領域のみでの線形性を反映。負の h 領域での振る舞いは未検証。ただし、GPT-2 small で rock, spring, bass は負の h を含み、同様のパターンを示しているため、定性的結論は変わらない。

---

## 結論

### ランダウ理論 vs 意味浸透モデル

| 予測 | ランダウ理論 | 意味浸透モデル | 実験結果 |
|------|-----------|-------------|---------|
| f(l) 形状 | (l-l_c)^β | 単調増加 | **単調増加** |
| β の普遍性 | β = 0.5 (平均場) | 語に依存 | **f(l) 形状は普遍的、スケールが語依存** |
| 線形応答 | 臨界点付近で発散 | 深い層ほど線形 | **深い層ほど線形** |
| 臨界点 | 特定の l_c | なし | **なし（漸進的浸透）** |

### 新しい知見

1. **f(l) の普遍性**: 語間相関 > 0.96 は、Transformer の「意味浸透関数」がアーキテクチャで決まることを示す。語の違いはスケーリング定数のみ。
2. **線形性の深さ依存性**: L0 では非線形、深い層では線形。文脈情報の「浸透」は初期層で最も複雑で、深い層では線形的に重ね合わされる。
3. **prior bias 効果**: GPT-2 medium ではB方向の弱い文脈が十分な σ 変化を生まない → モデルサイズが大きいほど prior が強い可能性。

### 次の実験への推奨

- **exp5（相図）**: 文脈長の連続変化で σ(context_length, layer) の2D相図を作成。f(l) の普遍性がここでも成立するか検証。
- **h 範囲の改善**: B方向プロンプトをより強い文脈にするか、異なる正規化スキーム（例: σ の中央値で正規化）を検討。

---

## 生成ファイル

### GPT-2 small
- `results/data/exp3_gpt2.json`
- `results/figures/exp3_sigma_vs_h_{word}_gpt2.png` (4語)
- `results/figures/exp3_linearity_{word}_gpt2.png` (4語)
- `results/figures/exp3_fl_comparison_gpt2.png`

### GPT-2 medium
- `results/data/exp3_gpt2-medium.json`
- `results/figures/exp3_sigma_vs_h_{word}_gpt2-medium.png` (4語)
- `results/figures/exp3_linearity_{word}_gpt2-medium.png` (4語)
- `results/figures/exp3_fl_comparison_gpt2-medium.png`
