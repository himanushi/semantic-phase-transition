# Experiment 7: Residual Stream Δφ PCA

## 背景と動機

exp3 で発見した普遍浸透関数 g(l/L)（語間相関>0.96）は、各レイヤーの「書き込み」パターンに強い規則性があることを示唆する。exp6 では静的な重み行列 W_QK の PCA を試みたが、低分散成分に機能的情報が集中しており圧縮は失敗した。

本実験では、各レイヤーが残差ストリームに**実際に書き込む差分** Δφ(l) = resid_post(l) - resid_pre(l) を PCA する。Δφ は動的（入力依存）であり、PCA の大きな方向が「頻繁に行われる書き込みの種類」= 機能的に重要な方向を捉える可能性がある。

## 実験設定

- モデル: GPT-2 small (12層, 12ヘッド, d_model=768)
- データ: WikiText-2 test set, 1000 トークン位置からΔφを収集
- デバイス: MPS (Apple Silicon)

---

## 7A: Δφ(l) の収集

各レイヤーの Δφ ノルム統計:

| Layer | mean_norm | std_norm |
|-------|-----------|----------|
| 0     | 50.4      | 4.8      |
| 1     | 16.3      | 15.6     |
| 2     | 18.0      | **73.1** |
| 3     | 17.6      | 6.2      |
| 4     | 18.3      | 5.1      |
| 5     | 23.6      | 5.8      |
| 6     | 26.0      | 5.1      |
| 7     | 36.9      | 8.4      |
| 8     | 36.5      | 5.3      |
| 9     | 52.4      | 9.1      |
| 10    | 97.9      | 12.9     |
| 11    | **354.1** | **118.0**|

**観察**: Δφ ノルムは深い層ほど大きい（Layer 0: 50 → Layer 11: 354）。Layer 2 と 11 は std/mean 比が異常に大きく、少数のサンプルが支配的。

---

## 7B: レイヤーごとの Δφ PCA

90% 累積寄与率に必要な基底数 K:

| Layer | 50%  | 80%  | 90%    | 95%  |
|-------|------|------|--------|------|
| 0     | 4    | 41   | 85     | 132  |
| 1     | 1    | 29   | 88     | 155  |
| 2     | **1**| **1**| **1**  | 1    |
| 3     | 26   | 116  | **198**| 281  |
| 4     | 32   | 132  | **220**| 306  |
| 5     | 31   | 133  | **226**| 317  |
| 6     | 34   | 141  | **235**| 326  |
| 7     | 33   | 130  | **221**| 312  |
| 8     | 32   | 132  | **222**| 312  |
| 9     | 23   | 110  | 198    | 286  |
| 10    | 12   | 85   | 169    | 258  |
| 11    | **1**| **3**| **3**  | 11   |

**結論: Δφ は低ランクではない。**

- **中間層 (3-8)**: 90% に K=198~235 必要（d_model の 26~31%）。事実上フルランク
- **Layer 0**: K=85（11%）。embedding 近傍は比較的低ランク
- **Layer 2**: K=1。分散が1方向に極端に集中（std_norm=73 が原因）
- **Layer 11**: K=3。最終層の unembedding 寄りの書き込みも集中的

「50基底で90%以上」の基準は**全レイヤーで未達成**（Layer 2/11 を除く全層で K>80）。

---

## 7C: レイヤー間の共有基底

全レイヤーの Δφ を結合して PCA（12000×768 行列）:

| Threshold | Shared K |
|-----------|----------|
| 50%       | 1        |
| 80%       | 3        |
| 90%       | 6        |
| 95%       | 75       |

共有基底は見かけ上非常に低ランク（K=6 で90%）。しかしこれは **Layer 11 のノルムが支配的** なためである（354 vs 中間層17~52）。

**再構成品質（K個の共有基底、全レイヤー平均）:**

| K   | cos_sim | relative_L2 |
|-----|---------|-------------|
| 8   | 0.503   | 0.782       |
| 16  | 0.548   | 0.749       |
| 32  | 0.604   | 0.710       |
| 64  | 0.669   | 0.660       |
| 128 | 0.750   | 0.585       |
| 256 | 0.845   | 0.470       |

**結論: 共有基底の再構成品質は低い。** K=256 でも cos_sim=0.845、相対L2誤差=47%。共有PCA は Layer 11 のノルムに汚染されており、中間層の細かい構造を全く捉えられていない。

---

## 7D: 低ランク近似による推論

Δφ を上位 K 個の基底に射影して推論:

| K   | Individual PPL | Ind. Ratio | Shared PPL | Sh. Ratio |
|-----|----------------|------------|------------|-----------|
| 8   | 4,315          | 144.9x     | 4,052      | 136.0x    |
| 16  | 3,627          | 121.8x     | 11,420     | 383.4x    |
| 32  | 2,143          | 71.9x      | 13,796     | 463.2x    |
| 64  | 1,612          | 54.1x      | 12,860     | 431.7x    |
| 128 | 873            | 29.3x      | 9,511      | 319.3x    |
| 256 | 248            | 8.3x       | 5,322      | 178.7x    |
| 512 | 44.5           | **1.49x**  | 208        | 7.0x      |
| 768 | 29.8           | 1.00x      | 29.8       | 1.00x     |

Baseline perplexity: 29.79

### 解釈

**個別基底**: K=512（d_model の 67%）でようやく 1.49x。K=256（33%）では 8.3x でまだ壊滅的。これは exp6 の W_QK 結果（K=140/144 で 1.07x）と同じパターン: **少数の基底では機能的情報を捉えられない**。

**共有基底**: 壊滅的。K=32 で 463x。共有基底が Layer 11 に最適化されてしまい、他の層の Δφ を破壊している。K が増えるにつれ PPL が**増加する**区間すらある（K=8→32 で 136x→463x）。これは、Layer 11 に不要な基底が中間層で有害なノイズとして作用するため。

---

## 7E: Attention vs FFN の分離

Δφ_attn（attention output）と Δφ_ffn（FFN output）を個別に PCA:

| Layer | Attn K(90%) | FFN K(90%) | Attn/FFN 比 |
|-------|-------------|------------|-------------|
| 0     | 29          | 127        | 0.23        |
| 1     | 63          | 92         | 0.68        |
| 2     | 97          | **1**      | 97.0        |
| 3     | 123         | 194        | 0.63        |
| 4     | 118         | 217        | 0.54        |
| 5     | 108         | 245        | 0.44        |
| 6     | 108         | 242        | 0.45        |
| 7     | 108         | 240        | 0.45        |
| 8     | 102         | 227        | 0.45        |
| 9     | 85          | 202        | 0.42        |
| 10    | 70          | 151        | 0.46        |
| 11    | **3**       | 36         | 0.08        |

### 解釈

**Attention は FFN より一貫して低ランク。** 中間層で Attn は FFN の約半分の次元数で90%到達（比率 0.42~0.68）。

- **Layer 0**: Attn が特に低ランク（K=29）。初期層の attention は限定的なパターン
- **Layer 2**: FFN が K=1 で90%。Layer 2 の FFN は1方向に極端に集中（7B で全体の Δφ が K=1 だったのはこの FFN が原因）
- **Layer 11**: Attn が K=3。最終層の attention は unembedding 方向への集中的な書き込み

**g(l/L) の規則性の起源**: Attention の書き込みが FFN より低ランクであることから、g(l/L) の規則性は **Attention 由来**の可能性が高い。Attention は各レイヤーで類似した低次元の書き込みパターンを持ち、これが「等量の文脈注入」として観測される。一方 FFN は高次元で多様な書き込みを行い、語彙的・意味的な処理を担当している。

---

## 全体的結論

### exp6 との比較

| 側面                    | exp6 (W_QK)      | exp7 (Δφ)               |
|------------------------|-------------------|--------------------------|
| 対象                    | 静的重み行列       | 動的活性化差分             |
| 90% に必要な K          | 82/144 (57%)     | 198-235/768 (26-31%)     |
| PPL ≈1.1x に必要な K   | 140/144 (97%)    | ~512/768 (67%)           |
| 共有基底の有効性         | N/A               | 壊滅的（ノルム不均衡）    |
| 結論                    | PCA 分散≠機能     | Δφ も高ランク、圧縮困難   |

### 核心的知見

1. **Δφ の PCA も圧縮に使えない**: 動的活性化空間でも、PCA の「大きな方向」≠「機能的に重要な方向」。exp6 と同じ教訓が別の角度から再確認された

2. **レイヤー間のスケール不均衡が共有基底を破壊**: Layer 11 のノルムは中間層の 20 倍。共有 PCA は大ノルム層に引きずられ、他の層を犠牲にする

3. **Attention は FFN より圧縮しやすい**: Attention の書き込みは FFN の約半分の次元で90%。もし圧縮を試みるなら、FFN ではなく Attention パスが候補

4. **Layer 2 と 11 は特異**:
   - Layer 2: FFN が1方向に集中（何らかの固定的バイアス項の可能性）
   - Layer 11: Attn が3方向に集中（unembedding への集約）
