# 実験3E/F/G: 普遍浸透関数 g(l/L) — 結果レポート

## 実験条件

- **入力データ**: exp3 の f(l) = dσ/dh（線形応答の傾き）
- **対象語**: 4語 (rock, spring, bass, light)
- **モデル**: GPT-2 small (12層), GPT-2 medium (24層)
- **実行日**: 2026-02-19

---

## 主要な発見

### 1. g(l/L) の普遍性が確認された（Part E）

σ(l,h) = h · f_max(word) · g(l/L) の分解において、
g_word(l/L) = f(l) / f_max の語間残差は小さい:

**GPT-2 small**:
| word | f_max | RMSE from g_mean | max|resid| |
|------|-------|------------------|------------|
| rock | 0.373 | 0.048 | 0.100 |
| spring | 0.309 | 0.031 | 0.064 |
| bass | 0.303 | 0.044 | 0.075 |
| light | 0.793 | 0.071 | 0.137 |

**GPT-2 medium**:
| word | f_max | RMSE from g_mean | max|resid| |
|------|-------|------------------|------------|
| rock | 0.387 | 0.033 | 0.060 |
| spring | 0.495 | 0.054 | 0.097 |
| bass | 0.500 | 0.055 | 0.128 |
| light | 0.548 | 0.046 | 0.078 |

語間の g_word 残差は RMSE < 0.08 で、実験3で確認した f(l) の語間相関 > 0.96 と整合的。
語間の違いはスケーリング定数 f_max のみで記述できる。

### 2. g(l/L) の「上昇→ドロップ」構造

g_mean(l/L) のプロットから明確なパターンが見える:

- **L0**: g ≈ 0（文脈情報なし）
- **L0 → L_peak**: 単調増加（意味浸透の主過程）
- **L_peak**: g ≈ 1.0
- **L_peak → L_final**: g が低下（unembedding 再構成の影響）

| モデル | L_peak (l/L) | g_peak | g_final | ドロップ幅 |
|--------|-------------|--------|---------|-----------|
| GPT-2 small | 0.833 (L10) | 0.993 | 0.743 | 0.249 |
| GPT-2 medium | 0.875 (L21) | 0.991 | 0.886 | 0.106 |

この「最終層ドロップ」は exp2 で発見した二段階構造の直接的表現。
f(l) = dσ/dh が最終層付近で減少するのは、unembedding 変換が h に対して
非線形的に作用することを意味する。

### 3. 全域フィット結果（Part F）

g_mean(l/L) を 5 つの候補関数でフィット:

**GPT-2 small** (13データ点):

| 順位 | Model | R² | AIC | BIC | パラメータ |
|------|-------|-----|-----|-----|-----------|
| 1 | **erf** | 0.9471 | -66.6 | -66.0 | σ=0.545 |
| 2 | log | 0.9283 | -62.7 | -62.1 | τ=0.546 |
| 3 | exponential | 0.9256 | -62.2 | -61.6 | τ=0.479 |
| 4 | sigmoid | 0.9366 | -62.2 | -61.1 | κ=5.43, xc=0.403 |
| 5 | power_law | 0.9200 | -61.2 | -60.6 | α=0.718 |

**GPT-2 medium** (25データ点):

| 順位 | Model | R² | AIC | BIC | パラメータ |
|------|-------|-----|-----|-----|-----------|
| 1 | **sigmoid** | 0.9814 | -151.3 | -148.9 | κ=6.16, xc=0.453 |
| 2 | log | 0.9561 | -131.9 | -130.7 | τ=1.529 |
| 3 | power_law | 0.9501 | -128.7 | -127.5 | α=0.864 |
| 4 | erf | 0.9443 | -126.0 | -124.7 | σ=0.591 |
| 5 | exponential | 0.8895 | -108.9 | -107.6 | τ=0.528 |

### 4. 最終層ドロップ除外フィット — 純粋浸透部分

最終2層を除外し、g を再正規化してフィット:

**GPT-2 small** (11データ点, l/L ≤ 0.833):

| 順位 | Model | R² | パラメータ |
|------|-------|-----|-----------|
| 1 | **power_law** | **0.9937** | α=0.913 |
| 2 | log | 0.9929 | τ=3.450 |
| 3 | erf | 0.9729 | σ=0.634 |
| 4 | sigmoid | 0.9709 | κ=4.84, xc=0.475 |
| 5 | exponential | 0.9358 | τ=0.564 |

**GPT-2 medium** (23データ点, l/L ≤ 0.917):

| 順位 | Model | R² | パラメータ |
|------|-------|-----|-----------|
| 1 | **sigmoid** | **0.9853** | κ=5.87, xc=0.489 |
| 2 | log | 0.9796 | τ=7.282 |
| 3 | power_law | 0.9787 | α=0.974 |
| 4 | erf | 0.9378 | σ=0.634 |
| 5 | exponential | 0.8772 | τ=0.573 |

**重要**: 最終層を除外すると、small では power_law (α=0.91 ≈ 1) が圧倒的に良く、
medium では sigmoid が最良だが power_law (α=0.97 ≈ 1) も同等。
**純粋な意味浸透はほぼ線形** (g ∝ l/L) に近い。

### 5. モデルサイズ間の比較（Part G）

| 指標 | 値 |
|------|-----|
| g_mean 相関 (small vs medium) | **0.982** |
| g_mean RMSE | 0.072 |
| g_mean max|diff| | 0.143 |

語ごとの cross-model 相関:
| word | 相関 |
|------|------|
| rock | 0.965 |
| spring | 0.986 |
| bass | 0.965 |
| light | 0.991 |

**全語・全ペアで相関 > 0.96**。l/L で正規化すると g は GPT-2 small と medium で
ほぼ一致し、モデルサイズにも依存しない普遍関数であることが確認された。

---

## 解釈

### erf フィットの意味

GPT-2 small の全域フィットでは erf (σ=0.545) が最良。
erf は拡散方程式 ∂σ/∂l = D · ∂²σ/∂x² の初期条件 σ(0)=0 からの解であり、
g(l/L) = erf(l/L / (σ√2)) は「意味の拡散係数」D ≈ σ²/2 ≈ 0.15 を定義できる。

ただし、erf が最良になるのは**最終層ドロップ**との整合性による面が大きい:
- erf は自然に l/L → 1 で飽和するため、ドロップ部分も部分的にフィットする
- 最終層を除外すると erf の優位性は消え、power_law (α≈1) に逆転

### 真の浸透関数の構造

実データは以下の二段階構造で記述される:

```
g(l/L) = { (l/L_peak)^α     (l/L < L_peak)    ... 純粋浸透 (α ≈ 0.9-1.0)
          { ドロップ         (l/L > L_peak)    ... unembedding 再構成
```

ここで L_peak/L ≈ 0.83-0.88。

**物理的解釈**:
1. **L0 → L_peak**: 意味情報がほぼ線形にレイヤーを通じて浸透。
   各レイヤーが一定量の文脈情報を注入する「定速浸透」。
2. **L_peak → L_final**: unembedding 方向への再構成が起こり、
   残差ストリームの方向が変化。この変化は h に対して非線形的に作用するため、
   f(l) = dσ/dh が減少する。

### 拡散仮説の現状

| 予測 | 拡散方程式 | 実データ | 判定 |
|------|-----------|---------|------|
| g の関数形 | erf | erf (全域) / power_law (truncated) | **部分的** |
| σ パラメータ | モデル不変 | small=0.545, medium=0.591 | **概ね一致** |
| g のモデル間一致 | 一致 | 相関 0.98 | **一致** |

erf は全域では最良だが、最終層を除くと power_law に負ける。
これは意味浸透が**単純拡散ではなく、ほぼ均一な定速浸透**であることを示唆する。
「意味の拡散係数」は定義可能だが、その物理的基盤は拡散方程式というよりは
残差ストリームへの逐次的加算に近い。

---

## σ(l, h) の完全分解

実験3全体の結果を統合すると:

```
σ(l, h) = h · f_max(word) · g(l/L)
```

ここで:
- **h**: 文脈強度（最終層 σ で定義、[-1, 1] に正規化）
- **f_max(word)**: 語依存のスケーリング定数（0.30 〜 0.79）
- **g(l/L)**: 普遍浸透関数。語・モデルサイズに非依存（語間相関 > 0.96, モデル間相関 0.98）

g の構造:
- l/L ∈ [0, 0.85]: g ≈ (l/L)^0.9 （ほぼ線形の定速浸透）
- l/L ∈ [0.85, 1.0]: g がドロップ（unembedding 再構成効果）

---

## 制限事項

1. **最終層ドロップの解釈**: f(l) のドロップは σ 自体のドロップではなく、
   σ vs h の**傾き**のドロップ。最終層では σ(h) 関係がより非線形的になることを意味。
2. **h 範囲の片側偏り**: GPT-2 medium では h > 0 のみ（exp3 からの制限）。
3. **4語のみ**: 普遍性の主張は4語での検証。9語全てでの検証が望ましい。
4. **GPT-2 のみ**: 他のアーキテクチャ（Llama, Pythia 等）での検証が必要。

---

## 結論

1. **g(l/L) の普遍性を確認**: 語間残差 RMSE < 0.08、モデル間相関 0.98
2. **erf が全域でベスト (small)**: σ ≈ 0.55 → 形式的に D ≈ 0.15 の「拡散係数」を定義可能
3. **純粋浸透部分は power_law α ≈ 1**: 最終層を除くとほぼ線形（定速浸透）
4. **二段階構造の定量的記述**: 浸透部 (g ∝ l) + unembedding ドロップ
5. **σ(l,h) = h · f_max · g(l/L)** の完全分解が成立

---

## 生成ファイル

### データ
- `results/data/exp3ef_gpt2.json`
- `results/data/exp3ef_gpt2-medium.json`
- `results/data/exp3g_cross_model.json`

### 図
- `results/figures/exp3e_universal_g_gpt2.png` — 4語の g 重ね合わせ (small)
- `results/figures/exp3e_universal_g_gpt2-medium.png` — 同 (medium)
- `results/figures/exp3f_fit_gpt2.png` — 5モデルのフィット比較 (small)
- `results/figures/exp3f_fit_gpt2-medium.png` — 同 (medium)
- `results/figures/exp3g_cross_model_g.png` — small vs medium 比較
- `results/figures/exp3g_best_fit_overlay.png` — 両モデルのベストフィット重ね合わせ
